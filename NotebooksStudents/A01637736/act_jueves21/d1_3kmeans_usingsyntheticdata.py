# -*- coding: utf-8 -*-
"""D1_3Kmeans_UsingSyntheticData.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TZ05sRCuJ980_bEcKZuA8lCo_C53696T

# Activity: work with the iris dataset

    
1. Do clustering with the iris flower dataset to form clusters using as features the four features

2. Do clustering with the iris flower dataset to form clusters using as features the two petal measurements: Drop out the other two features

3. Do clustering with the iris flower dataset to form clusters using as features the two sepal measurements: Drop out the other two features

4. Which one provides the better grouping? Solve this using programming skills, e.g., compute performance metrics
"""

RunInColab = True     # (False: no  | True: yes)

# colab:
if RunInColab:
    # Mount your google drive in google colab
    from google.colab import drive
    drive.mount('/content/drive')

    # Define ruta del proyecto
    Ruta = "/content/drive/My Drive/Reto_Sistemas_Eléctricos/NotebooksProfessor/"

else:
    # Define ruta del proyecto
    Ruta = ""
    # Import the packages that we will be using
import numpy as np                  # For array
import pandas as pd                 # For data handling
import seaborn as sns               # For advanced plotting
import matplotlib.pyplot as plt
# Dataset url
url = Ruta + "datasets/iris/iris.csv"

# Load the dataset
colnames=['Sepal length', 'Sepal width', 'Petal length', 'Petal width','Type']
dataset = pd.read_csv(url, header=None, names=colnames)

# Print the data
dataset

dataset.shape
print("Renglones:",dataset.shape[0],"Columnas:",dataset.shape[1])

# Drop rows with NaN values if existing
dataset.dropna()

# Print the new shape
print(dataset.shape)

# Pairplot: Scatterplot of all variables
g = sns.pairplot(dataset, corner =True, diag_kind="kde")
g.map_lower(sns.kdeplot, levels=4, color=".2")
plt.show()

# Eliminar columna de strings
dataset_nueva = dataset.drop(dataset.columns[4], axis=1)

# Imprimir nueva dataset
print(dataset_nueva)

# Import sklearn KMeans
from sklearn.cluster import KMeans

# Define number of clusters
K  = 5 # Let's assume there are 2,3,4,5...? clusters/groups

# Create/Initialize the Kmeans box/object
km = KMeans(n_clusters=K, n_init="auto")

# Do K-means clustering (assing each point in the dataset to a cluster)
yestimated = km.fit_predict(dataset_nueva)
#yestimated = km.fit_predict(df[['x1','x2']] )
#yestimated = km.fit_predict(df[['x1','x3']] )

# Print estimated cluster of each point in the dataset
yestimated

# Add a new column to the dataset with the cluster information
dataset_nueva['yestimated'] = yestimated

dataset_nueva

# Print the Labels/Names of the existing clusters
dataset_nueva.yestimated.unique()

# Cluster centroides
km.cluster_centers_

# Sum of squared error (sse) of the final model
km.inertia_

# The number of iterations required to converge
km.n_iter_

# Get a dataframe with the data of each cluster
df0 = dataset_nueva[dataset_nueva.yestimated == 0]
df1 = dataset_nueva[dataset_nueva.yestimated == 1]
df2 = dataset_nueva[dataset_nueva.yestimated == 2]
df3 = dataset_nueva[dataset_nueva.yestimated == 3]
df4 = dataset_nueva[dataset_nueva.yestimated == 4]

# Number of points in each cluster
NpointsCluster0 = df0.shape[0]
NpointsCluster1 = df1.shape[0]
NpointsCluster2 = df2.shape[0]
NpointsCluster3 = df3.shape[0]
NpointsCluster4 = df4.shape[0]

# Print
print("Number of points in cluster 0: " + str(NpointsCluster0))
print("Number of points in cluster 1: " + str(NpointsCluster1))
print("Number of points in cluster 2: " + str(NpointsCluster2))
print("Number of points in cluster 3: " + str(NpointsCluster3))
print("Number of points in cluster 4: " + str(NpointsCluster4))

# Print total number of points
total_points = NpointsCluster0 + NpointsCluster1 + NpointsCluster2 + NpointsCluster3 + NpointsCluster4
print("Total number of points in the clusters: " + str(total_points))

df0 = dataset[yestimated == 0]
df1 = dataset[yestimated == 1]
df2 = dataset[yestimated == 2]
df3 = dataset[yestimated == 3]
df4 = dataset[yestimated == 4]

plt.scatter(df0['Sepal length'], df0['Sepal width'], label='Cluster 0', c='r', marker='o', s=32, alpha=0.3)
plt.scatter(df1['Sepal length'], df1['Sepal width'], label='Cluster 1', c='g', marker='o', s=32, alpha=0.3)
plt.scatter(df2['Sepal length'], df2['Sepal width'], label='Cluster 2', c='b', marker='o', s=32, alpha=0.3)
plt.scatter(df3['Sepal length'], df3['Sepal width'], label='Cluster 3', c='y', marker='o', s=32, alpha=0.3)
plt.scatter(df4['Sepal length'], df4['Sepal width'], label='Cluster 4', c='m', marker='o', s=32, alpha=0.3)

plt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:,1], color='black', marker='*', label='Centroides', s=256)

plt.title('Diagrama de dispersión (para cada cluster)')
plt.xlabel('Longitud del Sépalo')
plt.ylabel('Ancho del Sépalo')
plt.legend()

plt.show()

# Intialize a list to hold sum of squared error (sse)
sse = []

# Define values of k
k_rng = range(1,10)

# For each k
for k in k_rng:
    # Create model
    km = KMeans(n_clusters=k, n_init="auto")
    # Do K-means clustering
    km.fit_predict(dataset_nueva[['Sepal length','Sepal width']])
    # Save sse for each k
    sse.append(km.inertia_)

# Plot sse versus k
plt.plot(k_rng,sse, 'o-', markersize=8)

plt.title('Elbow plot')
plt.xlabel('K')
plt.ylabel('Sum of squared error')
plt.show()